{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# KERAS\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune your Neural Network and Save it\n",
    "\n",
    "üéØ **Goals of this challenge**\n",
    "\n",
    "1. ‚öôÔ∏è **Fine-tune the optimizer** of a Neural Network\n",
    "2. üíæ **Save**/**Load** a trained Neural Network\n",
    "\n",
    "üë©üèª‚Äçüè´ Now that you have solid foundations about what Neural Networks are, how to design their architecture, and how to prevent them from overfitting, let's take a closer look at the `.compile(loss = ?, metrics = ?, activation = ?)` part."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (0) The Boston Housing Dataset\n",
    "\n",
    "üìö `Tensorflow.Keras` has several built-in datasets that you can find [here](https://www.tensorflow.org/api_docs/python/tf/keras/datasets)\n",
    "\n",
    "üè† Out of those, we are going to use the **Boston Housing Dataset**.\n",
    "\n",
    "Our mission is to **predict the values of the houses in USD (thousands)**, and we will measure the performance of our models using the _Mean Absolute Error (MAE)_ metric."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.1) Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset:\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of the dataset: \n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.2) Quick Glance at the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of the houses' prices in the training set\n",
    "sns.histplot(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values and types of each feature:\n",
    "pd.DataFrame(X_train).info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics about the numerical columns\n",
    "pd.DataFrame(X_train).describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.3) Minimal Data Preprocessing\n",
    "\n",
    "üëâ Here, we don't have any duplicates or missing values. Let's do the bare minimum of data preprocessing, i.e. ***scaling**, and move on quickly to the modeling phase."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ùìScaling your Features\n",
    "\n",
    "Standardize `X_train` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (0.4) Baseline Model\n",
    "\n",
    "üßëüèª‚Äçüè´ In a regression task, the baseline model **always predicts the average value of `y_train`**\n",
    "\n",
    "<details>\n",
    "    <summary>Really?</summary>\n",
    "    \n",
    "- üêí  Yes, in most cases!\n",
    "- ‚ùóÔ∏è  Be aware that this is not the only possible way of building a baseline model\n",
    "- üíπ  In Time Series, the baseline model predicts the **last seen value**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ùìQuestion: what would be the performance of the baseline model here?\n",
    "\n",
    "Before running any Machine Learning algorithm or advanced Deep Learning Neural Networks, it would be great to establish a benchmark score that you are supposed to beat. Otherwise, what is the point of running a fancy algorithm if you cannot beat this benchmark score on the testing set (other than showing off)?\n",
    "\n",
    "Compute the Mean Absolute Error on the testing set using a \"dumb\" prediction of the mean value of `y_train`, computed on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìInitializing a Neural Network with a Specific Architecture\n",
    "\n",
    "Write a function called `initialize_model` that generates a Neural Network with 3 layers:\n",
    "- Input layer: **10 neurons**, `relu` activation function, and the appropriate input dimension\n",
    "- Hidden layer: **7 neurons** and the `relu` activation function\n",
    "- Predictive layer: an appropriate layer corresponding to the problem we are trying to solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ùìNumber of Parameters\n",
    "\n",
    "How many parameters do we have in this model? \n",
    "1. Compute this number yourself\n",
    "2. Double-check your answer with `model.summary()`\n",
    "\n",
    "We already covered the question about the number of parameters in a fully connected/dense network during **Deep Learning > 01. Fundamentals of Deep Learning** but it is always good to make sure you master the foundations of a new discipline üòâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üí°Answer\n",
    "\n",
    "<details>\n",
    "    <summary>Click here</summary>\n",
    "\n",
    "- Each house has `X_train.shape[-1]` = 13 features\n",
    "- Remember that a neuron is a linear regression combined with an activation function, so we will have 13 weights and 1 bias\n",
    "\n",
    "1. First layer: **10 neurons** $\\times$ (13 weights + 1 bias ) = 140 params\n",
    "2. Second layer: **7 neurons** $\\times$ (10 weights + 1 bias ) = 77 params\n",
    "3. Third layer: **1 neuron** $\\times$ (7 weights + 1 bias) = 8 params\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Optimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìCompiling Method\n",
    "\n",
    "Write a function that:\n",
    "1. takes _both_ a **model** and an **optimizer** as arguments\n",
    "2. **compiles** the model\n",
    "3. returns the compiled model\n",
    "\n",
    "Please select wisely:\n",
    "- the **Loss Function** to be optimized\n",
    "- the **metrics** on which the model should be evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, optimizer_name):\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìEvaluating the Model\n",
    "\n",
    "- Initialize the model and compile it with the `adam` optimizer\n",
    "- Fit it on the training data\n",
    "- Evaluate your model on the testing data\n",
    "\n",
    "Don't forget to use an Early Stopping criterion to avoid overfitting!\n",
    "\n",
    "<details>\n",
    "    <summary>Notes</summary>\n",
    "\n",
    "As we saw in the **\"How to prevent overfitting\"** challenge,  you could also use L2 penalties and Dropout Layers to prevent overfitting but:\n",
    "- Early Stopping is the easiest and quickest code to implement, you just declare `es = EarlyStopping()` and call it back in the `.fit()` step\n",
    "- The main goal of this challenge is to understand the **impact of the optimizer**, so stay focused üòâ\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network vs. Baseline\n",
    "\n",
    "Compare the MAE on the testing set between this Neural Network and the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_test_baseline = mean_absolute_error_test_baseline\n",
    "mae_test_neuralnet = res\n",
    "\n",
    "print(f\"The MAE on the test is {mae_test_neuralnet:.4f} for the Neural Network vs. {mae_test_baseline:.4f} for the baseline\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Which Optimizer is the Best? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ‚ùìTrying Different Optimizers\n",
    "\n",
    "Re-run the same model on the same data using different optimizers (in a `for` loop). \n",
    "\n",
    "For each optimizer:\n",
    "- üìâ Plot the history of the loss (MSE) and the metric (MAE)\n",
    "    - üéÅ We coded two functions: `plot_loss_mae` and `plot_loss_mse`. Which one should you use? Feel free to use it.\n",
    "- ‚úçÔ∏è Report the corresponding Mean Absolute Error\n",
    "- ‚è≥ Compute the time your Neural Net needed to fit the training set\n",
    "\n",
    "üìö [`tensorflow.keras.optimizers`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_mae(history):\n",
    "    # Setting figures\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13,4))\n",
    "\n",
    "    # Create the plots\n",
    "    ax1.plot(history.history['loss'])\n",
    "    ax1.plot(history.history['val_loss'])\n",
    "\n",
    "    ax2.plot(history.history['mae'])\n",
    "    ax2.plot(history.history['val_mae'])\n",
    "\n",
    "    # Set titles and labels\n",
    "    ax1.set_title('Model loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "\n",
    "    ax2.set_title('MAE')\n",
    "    ax2.set_ylabel('MAE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "\n",
    "    # Set limits for y-axes\n",
    "    ax1.set_ylim(ymin=0, ymax=200)\n",
    "    ax2.set_ylim(ymin=0, ymax=20)\n",
    "\n",
    "    # Generate legends\n",
    "    ax1.legend(['Train', 'Validation'], loc='best')\n",
    "    ax2.legend(['Train', 'Validation'], loc='best')\n",
    "\n",
    "    # Show grids\n",
    "    ax1.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax1.grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    ax2.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax2.grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_mse(history):\n",
    "    # Setting figures\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(13,4))\n",
    "\n",
    "    # Create the plots\n",
    "    ax1.plot(history.history['loss'])\n",
    "    ax1.plot(history.history['val_loss'])\n",
    "\n",
    "    ax2.plot(history.history['mse'])\n",
    "    ax2.plot(history.history['val_mse'])\n",
    "\n",
    "    # Set titles and labels\n",
    "    ax1.set_title('Model loss')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "\n",
    "    ax2.set_title('MSE')\n",
    "    ax2.set_ylabel('MSE')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "\n",
    "    # Set limits for y-axes\n",
    "    ax1.set_ylim(ymin=0, ymax=20)\n",
    "    ax2.set_ylim(ymin=0, ymax=200)\n",
    "\n",
    "    # Generate legends\n",
    "    ax1.legend(['Train', 'Validation'], loc='best')\n",
    "    ax2.legend(['Train', 'Validation'], loc='best')\n",
    "\n",
    "    # Show grids\n",
    "    ax1.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax1.grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    ax2.grid(axis=\"x\",linewidth=0.5)\n",
    "    ax2.grid(axis=\"y\",linewidth=0.5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network vs. Baseline - Part 2\n",
    "\n",
    "Are your predictions better than those of the baseline model you evaluated at the beginning of the notebook?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE on the testing set for different optimizers\n",
    "for optimizer, result in zip(['rmsprop', 'adam', 'adagrad'], results):\n",
    "    print(f\"The MAE on the test set with the {optimizer} optimizer is equal to {result:.2f}\")\n",
    "\n",
    "# MAE on the testing set for the baseline model    \n",
    "print(\"-\"*5)    \n",
    "print(f\"The MAE on the test set with the baseline model is equal to {mae_test_baseline:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üí°Answer\n",
    "\n",
    "<details>\n",
    "    <summary>Click me</summary>\n",
    "\n",
    "You can see that the Neural Network beat the baseline when using either `adam` or `rmsprop` as an optimizer but the result was significantly worse with the `adagrad` optimizer.\n",
    "\n",
    "**üëá The advice from the Deep Learning community is the following üëá:**\n",
    "\n",
    "üî• So far, our best-performing optimizer is `adam`. Maybe a mathematician specialized in numerical methods will find a better solver in the future but for the moment, Adam is your best friend and they have already been helping us achieve remarkable results.\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Steps for this Challenge\n",
    "\n",
    "üë©‚Äçüéì Do you remember the **Machine Learning > 04.Under The Hood** unit where we coded our **Gradient Descent** by choosing a specific `learning_rate`? It represents how slow/fast your algorithm learns. In other words, it controls the intensity of the change of the weights at each optimization of the NN, at each backpropagation!\n",
    "\n",
    "üöÄ Well, the **solvers** in Machine Learning and the **optimizers** in Deep Learning are advanced iterative methods relying on **hyperparameters**, and the `learning_rate` is one of them!\n",
    "\n",
    "ü§î How can I control this `learning_rate`?\n",
    "\n",
    "‚úÖ Instead of calling an optimizer with a string (\"adam\", \"rmsprop\", etc.), which uses a default value for the Learning Rate, we will call üìö [`tf.keras.optimizers`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers) objects üìö and tailor them to our needs.\n",
    "\n",
    "üßëüèª‚Äçüè´ Different Learning Rates have different consequences, as shown here: \n",
    "\n",
    "<img src=\"https://wagon-public-datasets.s3-eu-west-1.amazonaws.com/06-DL/02-Optimizer-loss-and-fitting/learning_rate.png\" alt=\"Learning rate\" height=300>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) The Influence of the Learning Rate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting an Optimizer with a Custom Learning Rate\n",
    "\n",
    "üìö [`tf.keras.optimizers.Adam`](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam)\n",
    "\n",
    "Instead of initializing the optimizer with a string, let's initialize an optimizer manually.\n",
    "\n",
    "- Instantiate an Adam optimizer with a Learning Rate of $ \\alpha = 0.1$\n",
    "    - Keep the other parameters at their default values\n",
    "- Use this optimizer in the `compile_model` function\n",
    "- Train/fit the model\n",
    "- Plot the history\n",
    "- Evaluate your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 1. Instantiating the Adam optimizer with a learning rate alpha = 0.1\n",
    "adam = Adam(learning_rate = 0.1)\n",
    "\n",
    "# 2. Initializing the model\n",
    "model = initialize_model()\n",
    "\n",
    "# 3. Compiling the model with the custom Adam optimizer\n",
    "model = compile_model(model, adam)\n",
    "\n",
    "# 4. Training the neural net\n",
    "es = EarlyStopping(patience = 10)\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train, \n",
    "    validation_split = 0.3,\n",
    "    shuffle = True,\n",
    "    batch_size=16, \n",
    "    epochs = 1_000,\n",
    "    callbacks = [es],\n",
    "    verbose = 0\n",
    ")\n",
    "\n",
    "# 5. Plot the history and evaluate the model\n",
    "res = model.evaluate(X_test_scaled, y_test)[1]\n",
    "res\n",
    "print(f'Mean absolute error with a learning rate of 0.1: {res:.4f}')\n",
    "plot_loss_mae(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìPlaying with Learning Rates\n",
    "\n",
    "Now, reproduce the same plots and results but for different Learning Rates.\n",
    "\n",
    "<details>\n",
    "    <summary>Remark</summary>\n",
    "\n",
    "There is a chance that the y-axis is too large for you to visualize some results with some Learning Rates. In that case, feel free to re-write the plot function to plot only the epochs $> 10$.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 2]\n",
    "results = []\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    pass  # YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) The Loss\n",
    "\n",
    "---\n",
    "\n",
    "‚ùóÔ∏èIt's important to clearly understand the **difference between losses and metrics**‚ùóÔ∏è\n",
    "\n",
    "- üèãüèª‚Äç‚ôÄÔ∏è The **Loss Functions** are computed **during the training procedure**\n",
    "    - For regression tasks, the classic Loss Functions are **(Root) Mean Squared Error** ((R)MSE), **Mean Absolute Error** (MAE), and **Mean Squared Logarithmic Error** (MSLE, as seen in the Kaggle challenge)\n",
    "    - For classification tasks, the classic Loss Functions are **Binary Crossentropy** (also known as LogLoss), **Categorical Crossentropy**, Hinge Loss, etc.\n",
    "- üßëüèª‚Äçüè´ The **metrics** are computed to evaluate your models, **after training them**!\n",
    "    - For regression tasks, common metrics are MSE, MAE, RMSE, Coefficient of Determination (R2), etc.\n",
    "    - For classification tasks, common metrics are Accuracy, Recall, Precision, and F1-Score\n",
    "- üëÄ Notice that some metrics can also be used as Loss Functions, as long as they are differentiable! (e.g. the **MSE**)\n",
    "\n",
    "If these notions are not clear, we strongly advise reviewing **Machine Learning > 03.Performance Metrics** and **Machine Learning > 05.Model Tuning**\n",
    "\n",
    "---\n",
    "\n",
    "‚è© Alright, after this reminder, let's move on:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìOptimizing a Model with a certain Loss Function\n",
    "\n",
    "- Run the same NN, once with `mae` as the loss, and once with `mse`\n",
    "- In both cases, compare `mae_train`, `mae_val`, `mse_train`, `mse_val`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí°Learnings\n",
    "\n",
    "ü§î When you work on this regression task, you want to achieve the lowest MAE in the testing set at the end, right? So why wouldn't we use it directly as a Loss Function that would decrease with the number of epochs? \n",
    "\n",
    "<details>\n",
    "    <summary>Answer</summary>\n",
    "\n",
    "Well, even the Deep Learning research community is still trying to answer these types of questions rigorously.\n",
    "\n",
    "One thing is sure: in Deep Learning, you will never really reach the \"global minimum\" of the true Loss Function (the one computed using your entire training set as one single batch). So, in your first model (minimizing the MAE loss), your global MAE minimum has clearly **not** been reached (otherwise you could never beat it). \n",
    "\n",
    "Why? It may well be that the minimization process of the second model has performed better. Maybe because the Loss Function \"energy map\" is \"smoother\" or more \"convex\" in the case of MSE loss? Or maybe your hyper-parameters are best suited to the MSE than to the MAE loss?\n",
    "\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4)  Saving and Loading a Trained Neural Network\n",
    "\n",
    "ü§Ø Imagine that you trained a complex Neural Network (many layers/neurons) on a huge dataset. The parameters of your Deep Learning Model (weights and biases) are now optimized and you would like to share these weights with a teammate who wants to predict a new data point. Would you give this person your notebook for them to run it and then predict the new data point? Hell no, we have a much better solution:\n",
    "- üíæ Save the weights of the optimized Neural Network\n",
    "- ü§ù Your friend/colleague/teammate/classmate can use them to predict a new data point"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìTraining a Good Model\n",
    "\n",
    "- Try to reach an MAE on the testing set that is lower than 5 (_feel free to re-create the architecture and redefine your compiling parameters in this section!_)\n",
    "    - Remember: we are predicting house prices, so a mistake of less than 5.000 USD is already good in the real estate industry)\n",
    "\n",
    "- Whether you managed to reach it or not, move on to the \"Saving a Model\" section after a few attempts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult(\n",
    "    'solution',\n",
    "    mae_test = mae_test\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìSaving a Model\n",
    "\n",
    "üìö [`tf.keras.models.save_model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/save_model)\n",
    "\n",
    "Save your model using the `.save_model(model, 'name_of_my_model')` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìLoading a Model\n",
    "\n",
    "üìö [`tf.keras.models.load_model`](https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model)\n",
    "\n",
    "- Load the model that you've just saved using `.load_model('name_of_your_model')` and store it into a variable called `loaded_model`\n",
    "- Evaluate it on the testing data to check that it gives the same result as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) (Optional) Exponential Decay\n",
    "\n",
    "‚ùóÔ∏èWarning‚ùóÔ∏è \n",
    "\n",
    "- This section is optional and for advanced practitioners\n",
    "- The next question is not essential and can be indeed skipped as many algorithms can be run without such optimization\n",
    "\n",
    "üßëüèª‚Äçüè´ Instead of keeping a fixed Learning Rate, you can change it from one iteration to the other, with the intuition that at first, you need a large Learning Rate to learn fast, and as the Neural Network converges and gets closer to the minimum of the Loss Function, you can decrease the value of the Learning Rate. This is called a **scheduler**. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùìThe Exponential Decay Scheduler\n",
    "\n",
    "Use the üìö [Exponential Decay Scheduler](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay) üìö in the `adam` optimizer, and run it on the previous data.\n",
    "\n",
    "Start with the following:\n",
    "\n",
    "```python\n",
    "initial_learning_rate = 0.001 # start with default Adam value\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "    # Every 5000 iterations, multiply the learning rate by 0.7\n",
    "    initial_learning_rate, decay_steps = 5000, decay_rate = 0.7,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[0] * 0.7 / 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÅ Congratulations!\n",
    "\n",
    "üíæ Do not forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move to the next challenge!\n",
    "\n",
    "---\n",
    "\n",
    "**Further reading after your bootcamp:**\n",
    "\n",
    "The **Boston Housing Dataset** was deprecated by Scikit-Learn for ethical reasons, and TensorFlow may also replace it in the future.\n",
    "\n",
    "A certain M. Carlisle wrote a 12-min read article called [\"Racist Data Destruction\"](https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8) to investigate this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
